{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper institutes\n",
    "This notebook is a monthly scraper used to retrieve information about condition in detention centers in Italy. To do so, it uses the id numbers of the various detention centers to navigate to the dedicated webpages with Selenium, store locally the html code of the page and then parse it using BeautifuSoup. The information is then stored in a pandas dataframe and saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from time import sleep\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect institutes id numbers\n",
    "df_institutes = pd.read_csv(f'../outputs/clean/institutes_info.csv')\n",
    "prison_ids = df_institutes['id_istituto'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grab the html code of the page\n",
    "async def get_html(prison_id):\n",
    "\n",
    "    await page.goto(f\"{BASE_URL}{prison_id}\")\n",
    "    print(\"Fetching \" + f\"{BASE_URL}{prison_id}\")\n",
    "    page_html = await page.content()\n",
    "        \n",
    "    return page_html\n",
    "\n",
    "# Function to extract table data\n",
    "def extract_table_data(h2_text):\n",
    "    # Step 3: Find the specific <h2> tag by its text\n",
    "    h2_tag = soup.find('h2', class_='h2 campoComplessoTitolo', string=h2_text)\n",
    "\n",
    "\n",
    "    # Step 4: Find the table immediately following the <h2> tag\n",
    "    if h2_tag:\n",
    "        table = h2_tag.find_next('table')\n",
    "        \n",
    "        # Step 5: Extract data from the table into a DataFrame\n",
    "        if table:\n",
    "            data_list = []  # List to hold the extracted data\n",
    "            rows = table.find_all('tr')\n",
    "            \n",
    "            # Extract headers from the first row\n",
    "            headers = []\n",
    "            if rows:\n",
    "                header_row = rows[0].find_all('th')\n",
    "                headers = [header.get_text(strip=True) for header in header_row]\n",
    "            \n",
    "            # Extract data from the rest of the rows\n",
    "            for row in rows[1:]:  # Start from the second row\n",
    "                columns = row.find_all(['td'])  # Only data cells\n",
    "                data = [col.get_text(strip=True) for col in columns]\n",
    "                data_list.append(data)  # Add the row data to the list\n",
    "\n",
    "            # Create a DataFrame with headers\n",
    "            df = pd.DataFrame(data_list, columns=headers)\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Table not found after the specified <h2> tag.\")\n",
    "    else:\n",
    "        print(f\"<h2> tag with text '{h2_text}' not found.\")\n",
    "    \n",
    "    return pd.DataFrame()  # Return an empty DataFrame if nothing is found\n",
    "\n",
    "def extract_date(soup, date_text):\n",
    "    # Police staff\n",
    "    target_span= soup.find('h2', string=date_text)\n",
    "    try:\n",
    "        span = target_span.find_next_sibling('span')\n",
    "        date = span.text.strip()\n",
    "    except:\n",
    "        date = 'NA'\n",
    "\n",
    "    return date\n",
    "\n",
    "def extract_info(soup):\n",
    "    institute_name = soup.find('h1', {'class': 'titoloIstituto'}).text.strip()\n",
    "    institute_type = soup.find('h3', {'class': 'titoloIstituto'}).text.strip()\n",
    "    return institute_name, institute_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.giustizia.it/giustizia/it/dettaglio_scheda.page?s=\"\n",
    "\n",
    "data = []\n",
    "\n",
    "updates = [\n",
    "    'dati aggiornati al ',\n",
    "    'personale polizia penitenziaria aggiornato al',\n",
    "    'personale amministrativo aggiornato al',\n",
    "    'data di aggiornamento spazi detentivi',\n",
    "    ]\n",
    "\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.firefox.launch()\n",
    "context = await browser.new_context(viewport={'width': 1280, 'height': 800})\n",
    "page = await context.new_page()\n",
    "\n",
    "for prison_id in prison_ids:\n",
    "    success = False\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            print(f\"Attempt number {attempt+1} at fetching data for institute id {prison_id}\")\n",
    "            html_content = await get_html(prison_id)\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            capienza_df = extract_table_data('Capienza e presenze')\n",
    "            spazi_df = extract_table_data('Stanze di detenzione')\n",
    "            personale_df = extract_table_data('Personale')\n",
    "\n",
    "            # Combine the DataFrames\n",
    "            merged_df = pd.concat([\n",
    "                capienza_df.reset_index(drop=True), \n",
    "                personale_df.reset_index(drop=True),\n",
    "                spazi_df.reset_index(drop=True)\n",
    "            ], axis=1)\n",
    "\n",
    "            institute_name, institute_type = extract_info(soup)\n",
    "\n",
    "            # Now assign the values to new columns in the DataFrame\n",
    "            merged_df['nome istituto'] = institute_name\n",
    "            merged_df['tipo istituto'] = institute_type\n",
    "\n",
    "            for update in updates:\n",
    "                merged_df[update] = extract_date(soup, update)\n",
    "\n",
    "            merged_df['id istituto'] = prison_id\n",
    "            merged_df.columns = merged_df.columns.str.strip()\n",
    "\n",
    "            data.append(merged_df)\n",
    "\n",
    "            success = True  # Mark as successful\n",
    "            print(\"Success!\")\n",
    "            print()\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for prison id {prison_id}. Error: {e}\")\n",
    "            if attempt < 4:  # Only reinitialize if not on the last attempt\n",
    "                print(\"Reinitializing browser...\")\n",
    "                await browser.close()\n",
    "                await asyncio.sleep(10)\n",
    "                browser = await playwright.firefox.launch()\n",
    "                context = await browser.new_context(viewport={'width': 1280, 'height': 800})\n",
    "                page = await context.new_page()\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to fetch data for prison id {prison_id} after 5 attempts.\")\n",
    "\n",
    "    await asyncio.sleep(5)  # Use asyncio.sleep for asynchronous sleep\n",
    "\n",
    "await browser.close()\n",
    "\n",
    "# Convert the collected data to a Pandas DataFrame\n",
    "final_df = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Reorder the DataFrame\n",
    "new_column_order = ['id istituto', 'nome istituto', 'tipo istituto'] + \\\n",
    "                    [col for col in final_df.columns if col not in ['id istituto', 'nome istituto', 'tipo istituto']]\n",
    "final_df = final_df[new_column_order]\n",
    "\n",
    "#Fixing dates\n",
    "for update in (u.strip() for u in updates):\n",
    "    final_df[update] = pd.to_datetime(final_df[update], dayfirst=True, errors='coerce')\n",
    "    final_df[update] = final_df[update].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_path = Path('../outputs/clean/institutes.csv')\n",
    "\n",
    "if old_data_path.exists():\n",
    "  old_data = pd.read_csv('../outputs/clean/institutes.csv')\n",
    "  combined_data = pd.concat([old_data, final_df], ignore_index=True)\n",
    "  combined_data = combined_data.drop_duplicates()\n",
    "  combined_data.to_csv('../outputs/clean/institutes.csv', index=False, encoding='UTF-8-sig')\n",
    "\n",
    "else:\n",
    "  final_df = final_df.drop_duplicates(inplace=True)\n",
    "  final_df.to_csv('../outputs/clean/institutes.csv', index=False, encoding='UTF-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
